---
layout: default
---

## Abstract
The past decade has included groundbreaking algorithmic improvements on the back of immense amount of data. For computer vision and perception, the field of deep learning with convolutional neural networks have seen results on super-human levels. One application area where perception is key is autonomous vehicles, where perception is the backbone of path planning and route exection. From a safety perspective, the route execution is a safety critical task, where wrongful interpretations of the surrounding environment can lead to dangerous situations. However, both the field of deep learning and safety lack ways of presenting realistic safety cases for the usage of deep learning in safety critical applications. 

During the duration of this thesis _Guidance on the Assurance of Machine Learning in Autonomous Systems_ (AMLAS) and _ISO 21448 - Safety Of The Intended Functionality_ (SOTIF) has been researched. Both documents provide a starting ground to validate performance of machine learning based systems in safety automotive applications in a structured manner. However, both are purposedly vague with regards to implementation details and testing evaluation metrics. From SOTIF, a key message is to consider a confusion matrix with four states: 1) known safe states, 2) known unsafe states, 3) unknown unsafe states and 4) unknown safe states. This objective share similarities with one studied issue from the deep learning field, which is the handling of uncertain or input samples outside of the scope of the model. The issue stems from the fact that every input sample provides an output prediction, and several sub fields exist to mitigate the impact of undesired inputs e.g. generative models and adversarial training, outlier training and outlier detection. 

This thesis has focused on utilizing the knowledge from the field of outlier detection as a method to estimate uncertainty and as a rejection strategy of predictions when operating on data that are too far from the training domain. The beneficial safety argumentation of this approach is that it informs the remainder of the system when an output of the model is uncertain and should be used with caution. The approach is referred to as _supervisor_ (and safety cage) throughout the papers and is motivated through the study of nominal performance through AUROC measures (Area on the receiver operating characteristics curve) and risk-versus-coverage metric that allows a user defined accepted risk threshold as a connection to safety requirements. 

## Thesis included papers
1. [*Automotive safety and machine learning: Initial results from astudy on how to adapt the ISO 26262 safety standard*](http://mrksbrg.com/wp-content/uploads/preprints/2018_SEFAIAS_ISO26262.pdf), **Jens Henriksson**, Markus Borg, Cristofer Englund. IEEE/ACM International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS) 2018  
**Abstract:** Machine learning (ML) applications generate a continuous streamof success stories from various domains. ML enables many novelapplications, also in safety-critical contexts. However, the func-tional safety standards such as ISO 26262 did not evolve to coverML. We conduct an exploratory study on which parts of ISO 26262represent the most critical gaps between safety engineering and MLdevelopment. While this paper only reports the rst steps towarda larger research endeavor, we report three adaptations that arecritically needed to allow ISO 26262 compliant engineering, andrelated suggestions on how to evolve the standard.

2. [*Towards Structured Evaluation of Deep Neural Network Supervisors*](https://arxiv.org/pdf/1903.01263.pdf), **Jens Henriksson**, Christian Berger, Markus Borg, Lars Tornberg, Cristofer Englund, Sankar Raman Sathyamoorthy, Stig Ursing. IEEE International Conference On Artificial Intelligence Testing (AITest) 2019  
**Abstract:** Deep Neural Networks (DNN) have improved the quality of several non-safety related products in the past years. However, before DNNs should be deployed to safety-critical applications, their robustness needs to be systematically analyzed. A common challenge for DNNs occurs when input is dissimilar to the training set, which might lead to high confidence predictions despite proper knowledge of the input. Several previous studies have proposed to complement DNNs with a supervisor that detects when inputs are outside the scope of the network. Most of these supervisors, however, are developed and tested for a selected scenario using a specific performance metric. In this work, we emphasize the need to assess and compare the performance of supervisors in a structured way. We present a framework constituted by four datasets organized in six test cases combined with seven evaluation metrics. The test cases provide varying complexity and include data from publicly available sources as well as a novel dataset consisting of images from simulated driving scenarios. The latter we plan to make publicly available. Our framework can be used to support DNN supervisor evaluation, which in turn could be used to motive development, validation, and deployment of DNNs in safety-critical applications.

3. [*Performance Analysis of Out-of-Distribution Detection on Trained Neural Networks*](https://github.com/jenshenriksson/jenshenriksson.github.io/blob/master/2019_IST_Preprint.pdf), **Jens Henriksson**, Christian Berger, Markus Borg, Lars Tornberg, Sankar Raman Sathyamoorthy,  Cristofer Englund. Under review for a special track in Journal of Information and Software Technology 2020  
**Abstract:** Deep Neural Networks have shown great promise in various fields. However, before deploying these neural networks, the models need to be tested for robustness. One common challenge occurs when the model is exposed to samples outside of the intended operating domain, which can yield outputs with high confidence despite no prior knowledge of the input. **Objective:** The aim of this paper is to investigate how the performance of detecting out-of-distribution samples changes for outlier detection methods, as a deep neural network becomes better on training samples.  **Method:** Our experimental setup defines comparable metrics and datasets that reflect the most common setups in related work. The experimental setup allows for a fair comparison of supervisors, i.e methods with the goal of detecting out-of-distribution samples to a deep neural network. In order to enhance the comparison, four different deep neural networks are compared with three different supervisors during different stages of training, to detect when the performance of the supervisors begins to deteriorate. **Results:** We find that all supervisors has increased outlier detection performance as the quality of the model improves. However, we also find that all supervisors inherit a large variation in performance, which is affected by small variations in the network parameters, as well requiring parameter tuning. We observe that understanding the relationship between training results and supervisor performance is crucial to improve the model's robustness and to indicate, what input samples require further measures to improve the robustness of a DNN. **Conclusion:** Analysing Deep Neural Networks for robustness is a challenging task. We show that small variations in model parameters can have large impact on out-of-distribution detection performance. This kind of model behavior needs to be addressed to allow for safety argumentation of how deep neural networks shall be tested. 

## Additional papers
* [*Performance Analysis of Out-of-Distribution Detection on Various Trained Neural Networks*](https://ieeexplore.ieee.org/abstract/document/8906748), **Jens Henriksson**, Christian Berger, Markus Borg, Lars Tornberg, Sankar Raman Sathyamoorthy, Cristofer Englund. 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) 2019 

* [*Controlled time series generation for automotive software-in-the-loop testing using GANs*](https://arxiv.org/pdf/2002.06611.pdf), Dhasarathy Parthasarathy, Karl Bäckström, **Jens Henriksson**, Sólrún Einarsdóttir. IEEE International Conference On Artificial Intelligence Testing (AITest) 2020


